{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = 'data/DSL-TRAIN.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252000\n"
     ]
    }
   ],
   "source": [
    "# First let's see how many examples we have to work with\n",
    "linecount = 0\n",
    "with open(dataset, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        linecount += 1\n",
    "print(linecount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's briefly examine two tokenizing methods, the treebank tokenizer and a simple RegEx based tokenizer\n",
    "with open(dataset, encoding='utf-8') as f:\n",
    "    text, lang = f.readline().split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '57,5',\n",
       " 'miliona',\n",
       " 'maloljetnih',\n",
       " 'djevojčica',\n",
       " 'prisilno',\n",
       " 'stupi',\n",
       " 'u',\n",
       " 'brak',\n",
       " 'širom',\n",
       " 'svijeta',\n",
       " ',',\n",
       " 'dok',\n",
       " 'čak',\n",
       " '40',\n",
       " 'odsto',\n",
       " 'od',\n",
       " 'tog',\n",
       " 'broja',\n",
       " 'čine',\n",
       " 'maloljetne',\n",
       " 'Indijke',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TreebankWordTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '57',\n",
       " ',5',\n",
       " 'miliona',\n",
       " 'maloljetnih',\n",
       " 'djevojčica',\n",
       " 'prisilno',\n",
       " 'stupi',\n",
       " 'u',\n",
       " 'brak',\n",
       " 'širom',\n",
       " 'svijeta',\n",
       " ',',\n",
       " 'dok',\n",
       " 'čak',\n",
       " '40',\n",
       " 'odsto',\n",
       " 'od',\n",
       " 'tog',\n",
       " 'broja',\n",
       " 'čine',\n",
       " 'maloljetne',\n",
       " 'Indijke',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FRIDAY\\Anaconda3\\envs\\spacy\\lib\\site-packages\\ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "columns = ['Text', 'Language']\n",
    "df_dataset = pd.read_csv(dataset, header=None, index_col=False, sep=r'\\t', encoding=\"utf-8\")\n",
    "df_dataset.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- 57,5 miliona maloljetnih djevojčica prisilno...</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- Nakon ovih kalkulacija, ubrzo je postalo jas...</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U okviru programa Modul Memorije Internacional...</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sagrađen je po istom principu kao i slični \"gr...</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kontroverzni biznismen Naser Kelmendi, koji se...</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Language\n",
       "0  - 57,5 miliona maloljetnih djevojčica prisilno...       bs\n",
       "1  - Nakon ovih kalkulacija, ubrzo je postalo jas...       bs\n",
       "2  U okviru programa Modul Memorije Internacional...       bs\n",
       "3  Sagrađen je po istom principu kao i slični \"gr...       bs\n",
       "4  Kontroverzni biznismen Naser Kelmendi, koji se...       bs"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- 57,5 miliona maloljetnih djevojčica prisilno stupi u brak širom svijeta, dok čak 40 odsto od tog broja čine maloljetne Indijke.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.loc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bs',\n",
       " 'es-AR',\n",
       " 'es-ES',\n",
       " 'es-PE',\n",
       " 'fa-AF',\n",
       " 'fa-IR',\n",
       " 'fr-CA',\n",
       " 'fr-FR',\n",
       " 'hr',\n",
       " 'id',\n",
       " 'my',\n",
       " 'pt-BR',\n",
       " 'pt-PT',\n",
       " 'sr']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_dataset.Language.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "es-AR Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "es-ES Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "es-PE Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "fa-AF Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "fa-IR Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "fr-CA Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "fr-FR Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "hr Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "id Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "my Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "pt-BR Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "pt-PT Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n",
      "sr Text        18000\n",
      "Language    18000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for lg in list(df_dataset.Language.unique()):\n",
    "    print(lg, df_dataset[(df_dataset['Language']==lg)].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgDict = {'bs': 'Balkan-Bosnian',\n",
    " 'es-AR': 'Spanish-Argentine',\n",
    " 'es-ES': 'Spanish-Spanish',\n",
    " 'es-PE': 'Spanish-Peruvian',\n",
    " 'fa-AF': 'Farsi-Dari',\n",
    " 'fa-IR': 'Farsi-Persian',\n",
    " 'fr-CA': 'French-Canadian',\n",
    " 'fr-FR': 'French-French',\n",
    " 'hr': 'Balkan-Croatian',\n",
    " 'id': 'Bahasa-Indonesian',\n",
    " 'my': 'Bahasa-Malaysian',\n",
    " 'pt-BR': 'Portuguese-Brazilian',\n",
    " 'pt-PT': 'Portuguese-Portuguese',\n",
    " 'sr': 'Balkan-Serbian'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/languages.txt', 'w') as lgfile:\n",
    "    lgfile.write(str(list(df_dataset.Language.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4b440a4e4618>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mng\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "ng = ngrams(tokens, 3)\n",
    "[\" \".join(x) for x in ng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bahasa', 'Indonesian']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgDict['id'].split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FRIDAY\\Anaconda3\\envs\\spacy\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\FRIDAY\\Anaconda3\\envs\\spacy\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\FRIDAY\\Anaconda3\\envs\\spacy\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\FRIDAY\\Anaconda3\\envs\\spacy\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\FRIDAY\\Anaconda3\\envs\\spacy\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "lgs = list(df_dataset.Language.unique())\n",
    "for lg in lgs:\n",
    "    df2 = df_dataset[(df_dataset['Language']==lg)]\n",
    "    df2['Code'] = df2['Language']\n",
    "    if '-' not in lg:\n",
    "        df2['Language'] = [lgDict[lg] for i in range(df2.shape[0])]\n",
    "        df2['Dialect'] = [lgDict[lg] for i in range(df2.shape[0])]\n",
    "    else:\n",
    "        df2['Language'] = [lgDict[lg].split('-')[0] for i in range(df2.shape[0])]\n",
    "        df2['Dialect'] = [lgDict[lg].split('-')[1] for i in range(df2.shape[0])]\n",
    "    filename = \"data/dataset-{}.csv\".format(lg)\n",
    "    df2.to_csv(filename, encoding=\"utf-16\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ن ی ر',\n",
       " 'ی ر و',\n",
       " 'ر و ه',\n",
       " 'و ه ا',\n",
       " 'ه ا ی',\n",
       " 'ا ی  ',\n",
       " 'ی   ن',\n",
       " '  ن ظ',\n",
       " 'ن ظ ا',\n",
       " 'ظ ا م',\n",
       " 'ا م ی',\n",
       " 'م ی  ',\n",
       " 'ی   س',\n",
       " '  س و',\n",
       " 'س و م',\n",
       " 'و م ا',\n",
       " 'م ا ل',\n",
       " 'ا ل ی',\n",
       " 'ل ی  ',\n",
       " 'ی   و',\n",
       " '  و  ',\n",
       " 'و   ا',\n",
       " '  ا ت',\n",
       " 'ا ت ح',\n",
       " 'ت ح ا',\n",
       " 'ح ا د',\n",
       " 'ا د ی',\n",
       " 'د ی ه',\n",
       " 'ی ه  ',\n",
       " 'ه   آ',\n",
       " '  آ ف',\n",
       " 'آ ف ر',\n",
       " 'ف ر ی',\n",
       " 'ر ی ق',\n",
       " 'ی ق ا',\n",
       " 'ق ا  ',\n",
       " 'ا   ب',\n",
       " '  ب ا',\n",
       " 'ب ا  ',\n",
       " 'ا   ب',\n",
       " '  ب ی',\n",
       " 'ب ی ر',\n",
       " 'ی ر و',\n",
       " 'ر و ن',\n",
       " 'و ن  ',\n",
       " 'ن   ر',\n",
       " '  ر ا',\n",
       " 'ر ا ن',\n",
       " 'ا ن د',\n",
       " 'ن د ن',\n",
       " 'د ن \\xa0',\n",
       " 'ن \\xa0  ',\n",
       " '\\xa0   ا',\n",
       " '  ا س',\n",
       " 'ا س ل',\n",
       " 'س ل ا',\n",
       " 'ل ا م',\n",
       " 'ا م گ',\n",
       " 'م گ ر',\n",
       " 'گ ر ا',\n",
       " 'ر ا ی',\n",
       " 'ا ی ا',\n",
       " 'ی ا ن',\n",
       " 'ا ن  ',\n",
       " 'ن   ا',\n",
       " '  ا ل',\n",
       " 'ا ل ش',\n",
       " 'ل ش ب',\n",
       " 'ش ب ا',\n",
       " 'ب ا ب',\n",
       " 'ا ب ،',\n",
       " 'ب ،  ',\n",
       " '،   ک',\n",
       " '  ک ن',\n",
       " 'ک ن ت',\n",
       " 'ن ت ر',\n",
       " 'ت ر ل',\n",
       " 'ر ل  ',\n",
       " 'ل   ش',\n",
       " '  ش م',\n",
       " 'ش م ا',\n",
       " 'م ا ر',\n",
       " 'ا ر ی',\n",
       " 'ر ی  ',\n",
       " 'ی   ا',\n",
       " '  ا ز',\n",
       " 'ا ز  ',\n",
       " 'ز   ش',\n",
       " '  ش ه',\n",
       " 'ش ه ر',\n",
       " 'ه ر ه',\n",
       " 'ر ه ا',\n",
       " 'ه ا ی',\n",
       " 'ا ی  ',\n",
       " 'ی   ج',\n",
       " '  ج ن',\n",
       " 'ج ن و',\n",
       " 'ن و ب',\n",
       " 'و ب ی',\n",
       " 'ب ی  ',\n",
       " 'ی   و',\n",
       " '  و  ',\n",
       " 'و   م',\n",
       " '  م ر',\n",
       " 'م ر ز',\n",
       " 'ر ز ی',\n",
       " 'ز ی  ',\n",
       " 'ی   س',\n",
       " '  س و',\n",
       " 'س و م',\n",
       " 'و م ا',\n",
       " 'م ا ل',\n",
       " 'ا ل ی',\n",
       " 'ل ی  ',\n",
       " 'ی   ر',\n",
       " '  ر ا',\n",
       " 'ر ا  ',\n",
       " 'ا   د',\n",
       " '  د ر',\n",
       " 'د ر  ',\n",
       " 'ر   د',\n",
       " '  د س',\n",
       " 'د س ت',\n",
       " 'س ت  ',\n",
       " 'ت   گ',\n",
       " '  گ ر',\n",
       " 'گ ر ف',\n",
       " 'ر ف ت',\n",
       " 'ف ت ن',\n",
       " 'ت ن د',\n",
       " 'ن د .']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng = ngrams(df_read.loc[0]['Text'], 3)\n",
    "[\" \".join(x) for x in ng]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (spacy)",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "543px",
    "left": "1536px",
    "right": "20px",
    "top": "126px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
